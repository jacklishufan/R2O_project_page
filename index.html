<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<style type="text/css">
    table {
        margin-bottom: 5px;
        margin-top: 5px;
    }
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 60%;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img {
        width: 100%
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link, a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35), /* The third layer shadow */ 15px 15px 0 0px #fff, /* The fourth layer */ 15px 15px 1px 1px rgba(0, 0, 0, 0.35), /* The fourth layer shadow */ 20px 20px 0 0px #fff, /* The fifth layer */ 20px 20px 1px 1px rgba(0, 0, 0, 0.35), /* The fifth layer shadow */ 25px 25px 0 0px #fff, /* The fifth layer */ 25px 25px 1px 1px rgba(0, 0, 0, 0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }


    #authors td {
        padding-bottom: 5px;
        padding-top: 30px;
    }
</style>


<head>
    <title>Refine and Represent: Region-to-Object Representation Learning</title>
    <meta property="og:title" content="Refine and Represent: Region-to-Object Representation Learning"/>
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jacklishufan.github.io/R2O_project_page/" />
    <meta property="og:image" content="https://jacklishufan.github.io/R2O_project_page/assets/teaser.png" />
    <meta property="og:description" content="Region-to-Object Representation Learning (R2O) unifies region-based and object-centric pretraining. R2O operates by training an encoder to dynamically refine region-based segments into object-centric masks and then jointly learns representations of the contents within the mask." />    
</head>

<body>
<br>
<center>
    <span style="font-size:36px">Refine and Represent: Region-to-Object Representation Learning</span>

    <br>
    <br>

        <!-- <span style="font-size:24px">CVPR 2022</span> -->


    <br>
    <br>
<!-- Akash Gokul, Konstantinos Kallidromitis, Shufan Li, Yusuke Kato, Kazuki Kozuka, Trevor Darrell, Colorado Reed -->
    <table align=center>
        <tr>
            <span style="font-size:24px"><a >Akash Gokul</a>*<sup>1</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://tech-ai.panasonic.com/en/researcher_introduction/048/">Konstantinos Kallidromitis</a>*<sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a >Shufan Li</a>*<sup>1</sup></span> &nbsp;            
        </tr><br>
        <tr>
            <span style="font-size:24px"><a >Yusuke Kato</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://tech-ai.panasonic.com/en/researcher_introduction/005/">Kazuki Kozuka</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><sup>1</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~cjrd/">Colorado J Reed</a><sup>1</sup></span> 
        </tr>
    </table>


<!--    <br><br><br>-->

<!--    <br><br>-->


    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:20px">
                        * Equal Contribution <br>
                        <sup>1</sup>Berkeley AI Research &nbsp;<sup>2</sup>Panasonic
                    </span>
                </center>
            </td>
        </tr>
    </table>
<!--    <br>-->


<!--    <span style="font-size:24px">Preprint. Under review.</span>-->



</center>
<!--<left>    <span style="font-size:18px"><sup>*</sup>Equally contributed</span> </left>-->
<center><img style="margin-top: 2rem;" src="assets/explainer.png" align="middle"></center>
<br>
    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:24px"><a href=""> [Paper]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='https://github.com/KKallidromitis/r2o'> [GitHub]</a></span>
                </center>
            </td>
            <!-- <td align=center>
                <center>
                        <span style="font-size:24px"><a href='data/bib.txt'> [Bibtex]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='https://colab.research.google.com/drive/1ByFXJClyzNVelS7YdT53_bMbwYeMoeNb?usp=sharing'> [Demo]</a></span>
                </center>
            </td> -->
        </tr>
    </table>

<br><br>
<hr>

<table align=center>
    <center><h1>Abstract</h1></center>
</table>

Recent works in self-supervised learning have demonstrated strong performance
on scene-level dense prediction tasks with either object-centric or region-based
correspondence objectives. In this paper, we present Region-to-Object Representation Learning (R2O) which unifies region-based and object-centric pretraining.
R2O operates by training an encoder to dynamically refine region-based segments
into object-centric masks and then jointly learns representations of the contents
within the mask. R2O uses a region refinement module to group small image regions, generated using a region-level prior, into larger regions that are semantically
similar to objects by clustering region-level features. As pretraining progresses,
R2O follows a region-to-object curriculum which encourages learning region-level
features early on and gradually progresses to train object-centric representations.
Representations learned using R2O lead to state-of-the art performance in semantic
segmentation for PASCAL VOC (+0.7 mIOU) and Cityscapes (+0.4 mIOU) and
instance segmentation on MS COCO (+0.3 APmk). Further, after pretraining on
ImageNet, our encoders surpass existing state-of-the-art in unsupervised
object segmentation on the Caltech-UCSD Birds 200-2011 dataset (+2.9 mIoU)
without any further training.


<br><br><br>

<hr>
<br>

<table align=center>
    <center><h1 id="model">Region to Object Mask Refinement</h1></center>
</table>
<p>
    Our goal is to find semantically meaningful segmentations of images while jointly learning good representations for underlying semantics. We formulate this goal as a bilevel optimization problem. Our approach is to
    iteratively perform a mask-refinement step that refines region-level priors into object-centric masks and a representation learning step that optimizes representational invariance for mask-level features. Throughout the process, the number of segments is gradually reduced and the models's receptive field evolves from small neighborhoods to object-centric masks.
</p>
<center><img src="assets/teaser.png" align="middle"></center>
<br><br>

<hr>
<br>

<table align=center>
    <center><h1 id="viz">Visualization</h1></center>
</table>
<p>
    We visualize the masks generated by R2O in the mask refinement step during the pretraining on ImageNet-1K after 100, 200
    and 300 epochs. We demonstrate early masks consist of random image segments which gradually
    become object-centric. This qualitative analysis affirms our assumption.
</p>
<center><img src="assets/vis.png" align="middle"></center>

<br><br>
<hr>
<br>

<!-- <table align=center>
    <center><h1>Paper</h1></center>
    <tr>
        <td><a href="https://arxiv.org/abs/2106.04550"><img class="layered-paper-big" style="height:175px; width: 150px; margin-bottom: 50px"
                                                                src="data/paper.png"/></a></td>
        <td><a href="https://arxiv.org/abs/2106.04550"></a></td>
        <td>
        <span style="font-size:14pt"> <i>DETReg: Unsupervised Pretraining with Region Priors for Object Detection</i><br>Amir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, <br>Gal Chechik, Anna Rohrbach, Trevor Darrell, Amir Globerson<br>

            Arxiv<br>
            Hosted on <a href="https://arxiv.org/abs/2106.04550">arXiv</a>
                </span>
            <br>
        </td>
        <span style="font-size:4pt"><a href="https://arxiv.org/abs/2106.04550"><br></a></span>
    </tr>
</table>
<hr>
    <br><br> -->

<table align=center>
    <center><h1>Related Works</h1></center>
    <tr>
        <td>If you found our work interesting, please also consider looking into some closely related works like <a href="https://arxiv.org/abs/2103.10957">Detcon</a> and <a href="https://arxiv.org/abs/2203.08777">Odin</a>.
        </td>
    </tr>
</table>

    <br><br>


<hr>
    <br><br>

<!-- <table align=center style="margin-bottom: 50px">
    <tr>
        <td>
            <left>
                <center><h1>Acknowledgements</h1></center>

We would like to thank Sayna Ebrahimi for helpful feedback and discussions. This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080). Prof. Darrellâ€™s group was supported in part by DoD including DARPA's XAI, LwLL, and/or SemaFor programs, as well as BAIR's industrial alliance programs. GC group was supported by the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). This work was completed in partial fulfillment for the Ph.D degree of the first author.
</left>
        </td>
    </tr>
</table> -->


</body>
</html>
